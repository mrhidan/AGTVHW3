{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: *Chernyshev Daniil*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Those how are making projects on artilces: half of you need to choose co-authorship graph, others - citation (but consider citation graph as undirected for simplicity)\n",
    "2. Use weighted graph so that it would give you ability to calculate embeddings in more appropriate way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initiallize your classification set as follows:\n",
    "    * Determine training and testing intervals on your time domain (for example, take a period $2000$-$2014$ as training period and $2015$-$2018$ as testing period)\n",
    "    * Pick pairs of nodes that **have appeared during training interval** but **had no links** during it\n",
    "    * These pairs form **positive** or **negative** examples depending on whether they have formed coauthorships **during the testing interval**\n",
    "    * You have arrived to binary classification problem.\n",
    "2. Construct feature space:\n",
    "    * Use at least 2 features based on neighborhood \n",
    "    * Use at least 2 fetures based on shortest path\n",
    "    * Use embedding representation of nodes' pairs (for example, node2vec)\n",
    "    * Use idea of time series features (with time lag)\n",
    "    * Use idea of change-point detection\n",
    "3. Choose at least $3$ classification algorithms and compare them in terms of Accuracy, Precision, Recall, F-Score (for positive class) and Mean Squared Error. Use k-fold cross-validation and average your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import gc\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "dim=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Train (check classification first!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to construct the training dataset. The features are folowing:\n",
    "* Jaccard coefficient\n",
    "* Resource allocation index\n",
    "* Jaccard coefficient lag\n",
    "* Resoure allocation index lag\n",
    "* Distance\n",
    "* Inverse distance - this defines the likelyhood of link\n",
    "* Density - describes changes in graph structure\n",
    "* Node2vec Hadamard embedding (edge embedding in node2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node2vec is slow algorithm, so to deal with its performance we should somehow shrink our graph for current period. It can be done by leaving only connected components that contain at least one node for which target is defined (which means node is present in future period). Since we have not much data on articles it is expected to have low positive class rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildfet(P, C, F, dim=16):\n",
    "    fil=C.nodes & F.nodes\n",
    "    lfil=C.nodes & P.nodes\n",
    "    \n",
    "    cc=nx.connected_component_subgraphs(C)\n",
    "    Gfil=nx.Graph()\n",
    "    for i in cc:\n",
    "        if fil & i.nodes:\n",
    "            Gfil=nx.compose(Gfil, i)\n",
    "    \n",
    "    ne=list(nx.non_edges(Gfil))\n",
    "    \n",
    "    #filter\n",
    "    nef=[i for i in ne if i[0] in fil and i[1] in fil]\n",
    "    lnef=[i for i in nef if i[0] in lfil and i[1] in lfil]\n",
    "    print(len(nef))\n",
    "    jc=nx.jaccard_coefficient(Gfil, nef)\n",
    "    ra=nx.resource_allocation_index(Gfil, nef)\n",
    "    \n",
    "    #lag\n",
    "    ljc=nx.jaccard_coefficient(P, lnef)\n",
    "    lra=nx.resource_allocation_index(P, lnef)\n",
    "    \n",
    "    den=nx.density(Gfil)\n",
    "    \n",
    "    node2vec = Node2Vec(Gfil, dimensions=dim, walk_length=30, num_walks=200, workers=1)\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4, workers=8)\n",
    "    edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "    kv=edges_embs.as_keyed_vectors()\n",
    "    data=[]\n",
    "    lj=next(ljc)\n",
    "    lr=next(lra)\n",
    "    for j,r,n in zip(jc, ra, nef):\n",
    "        a=n[0]\n",
    "        b=n[1]\n",
    "        try:\n",
    "            d=nx.astar_path_length(Gfil, a, b)\n",
    "            ad=1/d\n",
    "        except:\n",
    "            d=999999999\n",
    "            ad=0\n",
    "        if(lj!=None and a==lj[0] and b==lj[1]):\n",
    "            pj=lj\n",
    "            try:\n",
    "                lj=next(ljc)\n",
    "            except:\n",
    "                lj=None\n",
    "        else:\n",
    "            pj=(None,None,None)\n",
    "        if(lr!=None and a==lr[0] and b==lr[1]):\n",
    "            pr=lr\n",
    "            try:\n",
    "                lr=next(lra)\n",
    "            except:\n",
    "                lr=None\n",
    "        else:\n",
    "            pr=(None,None,None)\n",
    "        target=b in F[a]\n",
    "        data.append((target, a, b, j[2],r[2], pj[2], pr[2], d, ad, den) + tuple(edges_embs[(str(a), str(b))]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods=[(2010, 2011), (2013, 2013), (2014,2014), (2015, 2015), (2016, 2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|█████████████████████████████████████████| 1211/1211 [00:08<00:00, 150.67it/s]\n",
      "Generating walks (CPU: 1): 100%|█████████████████████████████████████████████████████| 200/200 [04:10<00:00,  1.23s/it]\n",
      "Generating edge features: 100%|██████████████████████████████████████████| 733866/733866.0 [00:03<00:00, 221633.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|████████████████████████████████████████████| 650/650 [00:24<00:00, 26.12it/s]\n",
      "Generating walks (CPU: 1): 100%|█████████████████████████████████████████████████████| 200/200 [02:28<00:00,  1.36it/s]\n",
      "Generating edge features: 100%|██████████████████████████████████████████| 211575/211575.0 [00:00<00:00, 215918.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|█████████████████████████████████████████| 1226/1226 [00:01<00:00, 757.41it/s]\n",
      "Generating walks (CPU: 1): 100%|█████████████████████████████████████████████████████| 200/200 [03:47<00:00,  1.16s/it]\n",
      "Generating edge features: 100%|██████████████████████████████████████████| 752151/752151.0 [00:03<00:00, 212881.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30058\n"
     ]
    }
   ],
   "source": [
    "res=[]\n",
    "for i in range(1,len(periods)-1):\n",
    "    P=nx.Graph()\n",
    "    C=nx.Graph()\n",
    "    F=nx.Graph()\n",
    "    P.add_weighted_edges_from(getAutGraph(*periods[i-1]))\n",
    "    C.add_weighted_edges_from(getAutGraph(*periods[i]))\n",
    "    F.add_weighted_edges_from(getAutGraph(*periods[i+1]))\n",
    "    rda=buildfet(P,C,F)\n",
    "    print(len(rda))\n",
    "    res+=rda\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This gives us training period from 2013 to 2015 (2016 is considered as future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./tdata1.csv', 'w', encoding=\"utf8\") as csvoutput:\n",
    "    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "    writer.writerows(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build for 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tper=[('2016', '2016'),('2017','2017'),('2018','2019')]\n",
    "test=[]\n",
    "for i in range(1,len(tper)-1):\n",
    "    P=nx.Graph()\n",
    "    C=nx.Graph()\n",
    "    F=nx.Graph()\n",
    "    P.add_weighted_edges_from(getAutGraph(*tper[i-1]))\n",
    "    C.add_weighted_edges_from(getAutGraph(*tper[i]))\n",
    "    F.add_weighted_edges_from(getAutGraph(*tper[i+1]))\n",
    "    rda=buildfet(P,C,F)\n",
    "    print(len(rda))\n",
    "    test+=rda\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test22.csv', 'w', encoding=\"utf8\") as csvoutput:\n",
    "    writer = csv.writer(csvoutput, lineterminator='\\n')\n",
    "    writer.writerows(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load our precomputed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tdata1.csv','r', encoding=\"utf8\") as csvinput:\n",
    "        reader = csv.reader(csvinput)\n",
    "        data = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['Target','A', 'B', 'Jaccard', 'Resource_allocation', 'Jaccard_lag1', 'Resource_alloc_lag1', 'Distance', 'AntiDistance','Density'] + ['hadamard_'+str(i) for i in range(16)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Orbis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(data, columns=cols).convert_objects(convert_numeric=True).fillna(0).replace('False', False).replace('True', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Jaccard</th>\n",
       "      <th>Resource_allocation</th>\n",
       "      <th>Jaccard_lag1</th>\n",
       "      <th>Resource_alloc_lag1</th>\n",
       "      <th>Distance</th>\n",
       "      <th>AntiDistance</th>\n",
       "      <th>Density</th>\n",
       "      <th>...</th>\n",
       "      <th>hadamard_6</th>\n",
       "      <th>hadamard_7</th>\n",
       "      <th>hadamard_8</th>\n",
       "      <th>hadamard_9</th>\n",
       "      <th>hadamard_10</th>\n",
       "      <th>hadamard_11</th>\n",
       "      <th>hadamard_12</th>\n",
       "      <th>hadamard_13</th>\n",
       "      <th>hadamard_14</th>\n",
       "      <th>hadamard_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>53879453700</td>\n",
       "      <td>7004454931</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>...</td>\n",
       "      <td>3.794715</td>\n",
       "      <td>7.450766</td>\n",
       "      <td>0.313761</td>\n",
       "      <td>-5.207485</td>\n",
       "      <td>8.924924</td>\n",
       "      <td>3.738052</td>\n",
       "      <td>4.559178</td>\n",
       "      <td>1.548265</td>\n",
       "      <td>-7.566571</td>\n",
       "      <td>-7.101907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>53879453700</td>\n",
       "      <td>57204744255</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.095002</td>\n",
       "      <td>-11.695877</td>\n",
       "      <td>0.156034</td>\n",
       "      <td>7.434962</td>\n",
       "      <td>-0.715112</td>\n",
       "      <td>0.135229</td>\n",
       "      <td>40.969475</td>\n",
       "      <td>0.575845</td>\n",
       "      <td>4.850762</td>\n",
       "      <td>1.741908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>53879453700</td>\n",
       "      <td>7202300001</td>\n",
       "      <td>0.00813</td>\n",
       "      <td>0.00641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.293894</td>\n",
       "      <td>-2.871058</td>\n",
       "      <td>0.276012</td>\n",
       "      <td>1.963976</td>\n",
       "      <td>7.659011</td>\n",
       "      <td>1.409847</td>\n",
       "      <td>17.247793</td>\n",
       "      <td>0.089201</td>\n",
       "      <td>8.318015</td>\n",
       "      <td>4.951419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>53879453700</td>\n",
       "      <td>7401738392</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106118</td>\n",
       "      <td>26.749517</td>\n",
       "      <td>0.449816</td>\n",
       "      <td>-3.809829</td>\n",
       "      <td>13.804201</td>\n",
       "      <td>3.075871</td>\n",
       "      <td>-18.389505</td>\n",
       "      <td>-0.684995</td>\n",
       "      <td>-17.652903</td>\n",
       "      <td>-3.019060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>53879453700</td>\n",
       "      <td>7004262579</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.454909</td>\n",
       "      <td>27.315493</td>\n",
       "      <td>0.790719</td>\n",
       "      <td>-8.232331</td>\n",
       "      <td>2.634415</td>\n",
       "      <td>4.641665</td>\n",
       "      <td>31.488245</td>\n",
       "      <td>-0.546559</td>\n",
       "      <td>0.624052</td>\n",
       "      <td>9.855749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target            A            B  Jaccard  Resource_allocation  \\\n",
       "0   False  53879453700   7004454931  0.00000              0.00000   \n",
       "1   False  53879453700  57204744255  0.00000              0.00000   \n",
       "2   False  53879453700   7202300001  0.00813              0.00641   \n",
       "3   False  53879453700   7401738392  0.00000              0.00000   \n",
       "4   False  53879453700   7004262579  0.00000              0.00000   \n",
       "\n",
       "   Jaccard_lag1  Resource_alloc_lag1   Distance  AntiDistance   Density  \\\n",
       "0           0.0                  0.0  999999999      0.000000  0.025373   \n",
       "1           0.0                  0.0          3      0.333333  0.025373   \n",
       "2           0.0                  0.0          2      0.500000  0.025373   \n",
       "3           0.0                  0.0  999999999      0.000000  0.025373   \n",
       "4           0.0                  0.0  999999999      0.000000  0.025373   \n",
       "\n",
       "      ...       hadamard_6  hadamard_7  hadamard_8  hadamard_9  hadamard_10  \\\n",
       "0     ...         3.794715    7.450766    0.313761   -5.207485     8.924924   \n",
       "1     ...        -4.095002  -11.695877    0.156034    7.434962    -0.715112   \n",
       "2     ...        -2.293894   -2.871058    0.276012    1.963976     7.659011   \n",
       "3     ...        -0.106118   26.749517    0.449816   -3.809829    13.804201   \n",
       "4     ...        -4.454909   27.315493    0.790719   -8.232331     2.634415   \n",
       "\n",
       "   hadamard_11  hadamard_12  hadamard_13  hadamard_14  hadamard_15  \n",
       "0     3.738052     4.559178     1.548265    -7.566571    -7.101907  \n",
       "1     0.135229    40.969475     0.575845     4.850762     1.741908  \n",
       "2     1.409847    17.247793     0.089201     8.318015     4.951419  \n",
       "3     3.075871   -18.389505    -0.684995   -17.652903    -3.019060  \n",
       "4     4.641665    31.488245    -0.546559     0.624052     9.855749  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test1.csv','r', encoding=\"utf8\") as csvinput:\n",
    "        reader = csv.reader(csvinput)\n",
    "        test = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Orbis\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "te=pd.DataFrame(test, columns=cols).convert_objects(convert_numeric=True).fillna(0).replace('False', False).replace('True', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=rf(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.fit(df[cols[3:]], df['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0138881 , 0.01120263, 0.        , 0.        , 0.00269907,\n",
       "       0.00319414, 0.01214652, 0.08655407, 0.06740997, 0.05960492,\n",
       "       0.05017548, 0.06017548, 0.0550288 , 0.07073436, 0.06109231,\n",
       "       0.05649842, 0.06525971, 0.05445577, 0.05194015, 0.05613828,\n",
       "       0.04165168, 0.05394132, 0.06620882])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr=cl.predict(te[cols[3:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kcl=kn(n_neighbors=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kcl.fit(df[cols[3:]], df['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnr=kcl.predict(te[cols[3:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl=svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scl.fit(df[cols[3:]], df['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmr=scl.predict(te[cols[3:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score as prs\n",
    "from sklearn.metrics import f1_score as f1s\n",
    "from sklearn.metrics import recall_score as rs\n",
    "from sklearn.metrics import accuracy_score as acs\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tocal=[rfr,knnr,svmr]\n",
    "calname=['Random Forest', 'KNN', 'SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Orbis\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Orbis\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "calculated=[]\n",
    "for i in tocal:\n",
    "    calculated.append((prs(te['Target'],i),rs(te['Target'],i),f1s(te['Target'],i),acs(te['Target'],i), mse(te['Target'].astype(np.float32),i.astype(np.float32))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp=pd.DataFrame(calculated, columns=['Precision', 'Recall', 'F1_Score', 'Accuracy', 'MSE'],index=calname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.993204</td>\n",
       "      <td>0.006796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.993204</td>\n",
       "      <td>0.006796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.993204</td>\n",
       "      <td>0.006796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Precision  Recall  F1_Score  Accuracy       MSE\n",
       "Random Forest        0.0     0.0       0.0  0.993204  0.006796\n",
       "KNN                  0.0     0.0       0.0  0.993204  0.006796\n",
       "SVM                  0.0     0.0       0.0  0.993204  0.006796"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something is strange. Lets take a look at positive class rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006795922446532081"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(te[te['Target']])/len(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very low..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00475136331731548"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['Target']])/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even lower!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And predictions are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = confusion_matrix(te['Target'], np.mean([rfr,knnr,svmr], axis=0))\n",
    "\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14907     0]\n",
      " [  102     0]]\n"
     ]
    }
   ],
   "source": [
    "print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basicaly, all algorithms are predicting only 0, which is right since the probability of 1 in training dataset is less than 1%. This means that having link is an anomalous event. Unfortunately, this gives us no posibillity to compare the actual accuracy of this classification problem. \n",
    "\n",
    "If we had more data, the graph would be more connected and the author cooperation would be more likely event. However, generating training dataset would be computationally expensive and would take weeks to build it (5k node coauthorship graph already results in about 0.5mil unlinked node pairs which requires about 6 hours to process). But maybe this is just python performance issue and using efficient languages like c++ would solve the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
